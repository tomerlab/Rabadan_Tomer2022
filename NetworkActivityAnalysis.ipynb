{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for analysis of network activity\n",
    "@uthor: Raju Tomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monnet_utils as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import glob\n",
    "import skimage.external.tifffile as tff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import caiman as cm\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour, inspect_correlation_pnr\n",
    "import re\n",
    "import pickle\n",
    "import scipy as spy\n",
    "import skimage.transform as skt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "from scipy import ndimage as ndi\n",
    "from skimage.transform import downscale_local_mean\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "#import community\n",
    "#import pyintergraph\n",
    "#from cdlib import algorithms\n",
    "import scipy.io as sio\n",
    "import multiprocessing as mp\n",
    "\n",
    "import scipy\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Core data location sections\n",
    "### Load different batches of data (from different directories) as dictionary\n",
    "\n",
    "def read_init_dat(data_set):\n",
    "    if (data_set == 'CD1'):\n",
    "        analysis_dir = r'X:\\People\\Raju\\CD1'\n",
    "        f_data_dir = r'Y:\\CD1\\Results'\n",
    "        dict_im_lab = tl.load_pickle(f_data_dir,  r'dict_im_lab-CD1-SM.pickle')\n",
    "        dict_in_fn = tl.load_pickle(f_data_dir, r'dict_in_fn-CD1-SM.pickle')\n",
    "    elif (data_set == 'C57'):\n",
    "        analysis_dir = r'X:\\People\\Raju\\C57'\n",
    "        f_data_dir = r'Y:\\C57\\Results'\n",
    "        dict_im_lab = tl.load_pickle(f_data_dir,  r'dict_im_lab-C57.pickle')\n",
    "        dict_in_fn = tl.load_pickle(f_data_dir, r'dict_in_fn-C57.pickle')\n",
    "    elif (data_set == 'Df16'):\n",
    "        analysis_dir = r'X:\\People\\Raju\\DF16'\n",
    "        f_data_dir = r'Y:\\Data\\Results'\n",
    "        dict_im_lab = tl.load_pickle(f_data_dir,  r'dict_im_lab-DF16-.pickle')\n",
    "        dict_in_fn = tl.load_pickle(f_data_dir, r'dict_in_fn-DF16.pickle')\n",
    "    elif (data_set == r'Setd1'):\n",
    "        analysis_dir = r'X:\\People\\Raju\\Setd1'\n",
    "        f_data_dir = r'Y:\\Setd1\\Results'\n",
    "        dict_im_lab = tl.load_pickle(f_data_dir,  r'dict_im_lab-SETD1.pickle')\n",
    "        dict_in_fn = tl.load_pickle(f_data_dir, r'dict_in_fn-SETD1.pickle')\n",
    "\n",
    "    dict_dff_NsP = tl.load_pickle(analysis_dir, 'dict_dff_NSp.pickle')\n",
    "\n",
    "    file_names_nsp_ids = sorted(glob.glob(os.path.join(analysis_dir, 'nsp_ids_key*')))\n",
    "    dict_nsp_ids_fn = tl.construct_fn_dict(file_names_nsp_ids, key_extr_pat=r'.*nsp_ids_key(\\d*)_(.*\\.tif)\\..*')\n",
    "    file_names_cords_A = sorted(glob.glob(os.path.join(analysis_dir, 'cords_A_key*')))\n",
    "    dict_cords_fn = tl.construct_fn_dict(file_names_cords_A, key_extr_pat=r'.*cords_A_key(\\d*).*')\n",
    "    dict_nsp_labs = {}\n",
    "    dict_com_neuron = {}\n",
    "    for k,f in dict_nsp_ids_fn.items():\n",
    "#         print('Reading: ', k, f)\n",
    "        dict_nsp_labs[k] = np.load(os.path.join(analysis_dir,f))\n",
    "    for k,f in dict_cords_fn.items():\n",
    "#         print('Reading: ', k, f)\n",
    "        dict_com_neuron[k] = np.load(os.path.join(analysis_dir,f))\n",
    "\n",
    "    dict_dff = tl.load_pickle(analysis_dir, 'dict_dff.pickle')\n",
    "    dict_dff_zsc = tl.load_pickle(analysis_dir, 'dict_dff_zsc.pickle')\n",
    "\n",
    "    return analysis_dir, dict_dff_NsP, dict_im_lab, dict_in_fn, dict_nsp_labs, dict_com_neuron, dict_dff, dict_dff_zsc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Read all the data\n",
    "\n",
    "ad_C57, d_dffNsp_C57, d_labIm_C57, d_fn_C57, d_nspIDs_C57, d_com_C57, d_dff_C57, d_dffZsc_C57 = read_init_dat('C57')\n",
    "print('C57 Done')\n",
    "\n",
    "ad_CD1, d_dffNsp_CD1, d_labIm_CD1, d_fn_CD1, d_nspIDs_CD1, d_com_CD1, d_dff_CD1, d_dffZsc_CD1 = read_init_dat('CD1')\n",
    "print('CD1 Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Plotting Dff traces example ####\n",
    "# for Fig 01 example in panel 1\n",
    "\n",
    "begin_tp = 0 ### begin skip\n",
    "end_tp = -1  ### -1 mean no skipping\n",
    "dff_cut = 0.0\n",
    "norm_corr = False\n",
    "fig_dir = r'W:\\People\\Raju\\NNet_Paper\\Figures\\Fig01_dat'\n",
    "save_fig = False\n",
    "# s_zsc_th = 5\n",
    "# # zsc_th = 0.25\n",
    "# do_gauss_smooth = True\n",
    "# gauss_ker = 5\n",
    "fps=30\n",
    "max_zsc_int = .75\n",
    "for k in d_dff_CD1.keys():\n",
    "    if (k == 195):\n",
    "        fn = d_fn_CD1[k]\n",
    "        print(fn)\n",
    "        nsp_ids = d_nspIDs_CD1[k]\n",
    "        nsp_ids_filt = nsp_ids[nsp_ids>0]\n",
    "        dat = np.copy(d_dff_CD1[k])\n",
    "        im_lab = d_labIm_CD1[k]\n",
    "        coms = d_com_CD1[k][nsp_ids>0,:]\n",
    "        dist_pw = scipy.spatial.distance.pdist(coms, 'euclidean')\n",
    "        dist_pw = scipy.spatial.distance.squareform(dist_pw)\n",
    "    #         im_max = tff.imread(fn1)\n",
    "        post_str = 'k' + str(k) + '_' + os.path.basename(fn) + '.png'\n",
    "    ####### plot dff\n",
    "        if (1):\n",
    "            sz_per_neuron = 0.025\n",
    "            gr_ht = np.maximum(1,int(dat.shape[0]*sz_per_neuron))\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(7/2,gr_ht/2), sharey=True) \n",
    "    #             dff_th = 1\n",
    "    #             dat1 = dat1 - dff_th\n",
    "    #             dat1[dat1<0] = 0\n",
    "    #             dat2 = dat2 - dff_th\n",
    "    #             dat2[dat2<0] = 0\n",
    "            n_start = 240\n",
    "            n_stop = 265\n",
    "            tl.draw_activity_comp_in_range(dat, ax, nsp_ids_filt, max_zsc_int, \n",
    "                                             begin_tp = begin_tp, end_tp=end_tp, dff_bar=1,\n",
    "                                           fps=30, n_start=n_start, n_stop=n_stop, lw=.55)\n",
    "    #         tl.draw_activity_comp(dat2, ax2, nsp_ids_filt, max_zsc_int, \n",
    "    #                                          begin_tp = begin_tp, end_tp=end_tp, dff_bar=1, fps=30)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            if (save_fig):\n",
    "                fig_fn = os.path.join(fig_dir,'dff_k' + str(k) + '_' + str(n_start) + '_' + str(n_stop) + '.png')\n",
    "                print(fig_fn)\n",
    "                fig.savefig(fig_fn, dpi=300, bbox_inches=\"tight\")            \n",
    "                fig_fn = os.path.join(fig_dir,'dff_k' + str(k) + '_' + str(n_start) + '_' + str(n_stop) + '.eps')\n",
    "                print(fig_fn)\n",
    "                fig.savefig(fig_fn, dpi=300, bbox_inches=\"tight\")            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_s_c(d_dff, p=2, dff_th=0.0):\n",
    "    d_s = {}\n",
    "    d_c = {}\n",
    "    d_s_zsc = {}\n",
    "    for k in d_dff.keys():\n",
    "        if(k%50 == 0):\n",
    "            print(k)\n",
    "        dff = np.copy(d_dff[k])\n",
    "        dff = dff + 0.0001\n",
    "#         dff[dff<dff_th] = 0\n",
    "        try:\n",
    "            d_c[k], d_s[k] = tl.convert_f_to_cs(dff, p=p)\n",
    "            d_s_zsc[k] = spy.stats.zscore(np.copy(d_s[k]), axis=1)\n",
    "        except:\n",
    "            print('Error in key: ', k)\n",
    "            continue\n",
    "    return d_c, d_s, d_s_zsc\n",
    "\n",
    "if(1):\n",
    "    d_dff_c_CD1_p2, d_dff_s_CD1_p2, d_dff_s_zsc_CD1_p2 = calc_s_c(d_dff_CD1, p=2, dff_th=0.0)\n",
    "    tl.save_pickle(ad_CD1, 'd_dff_c_CD1_p2.pickle', d_dff_c_CD1_p2)\n",
    "    tl.save_pickle(ad_CD1, 'd_dff_s_CD1_p2.pickle', d_dff_s_CD1_p2)\n",
    "    tl.save_pickle(ad_CD1, 'd_dff_s_zsc_CD1_p2.pickle', d_dff_s_zsc_CD1_p2)\n",
    "    print('Done1')\n",
    "\n",
    "    d_dff_c_C57_p2, d_dff_s_C57_p2, d_dff_s_zsc_C57_p2 = calc_s_c(d_dff_C57, p=2, dff_th=0.0)\n",
    "    tl.save_pickle(ad_C57, 'd_dff_c_C57_p2.pickle', d_dff_c_C57_p2)\n",
    "    tl.save_pickle(ad_C57, 'd_dff_s_C57_p2.pickle', d_dff_s_C57_p2)\n",
    "    tl.save_pickle(ad_C57, 'd_dff_s_zsc_C57_p2.pickle', d_dff_s_zsc_C57_p2)\n",
    "    print('Done2')\n",
    "\n",
    "    d_dff_c_Setd1_p2, d_dff_s_Setd1_p2, d_dff_s_zsc_Setd1_p2 = calc_s_c(d_dff_Setd1, p=2, dff_th=0.0)\n",
    "    tl.save_pickle(ad_Setd1, 'd_dff_c_Setd1_p2.pickle', d_dff_c_Setd1_p2)\n",
    "    tl.save_pickle(ad_Setd1, 'd_dff_s_Setd1_p2.pickle', d_dff_s_Setd1_p2)\n",
    "    tl.save_pickle(ad_Setd1, 'd_dff_s_zsc_Setd1_p2.pickle', d_dff_s_zsc_Setd1_p2)\n",
    "    print('Done3')\n",
    "\n",
    "    d_dff_c_Df16_p2, d_dff_s_Df16_p2, d_dff_s_zsc_Df16_p2 = calc_s_c(d_dff_Df16, p=2, dff_th=0.0)\n",
    "    tl.save_pickle(ad_Df16, 'd_dff_c_Df16_p2.pickle', d_dff_c_Df16_p2)\n",
    "    tl.save_pickle(ad_Df16, 'd_dff_s_Df16_p2.pickle', d_dff_s_Df16_p2)\n",
    "    tl.save_pickle(ad_Df16, 'd_dff_s_zsc_Df16_p2.pickle', d_dff_s_zsc_Df16_p2)\n",
    "    print('Done4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (0):   #Run one time only. Afterwards pickles are loaded as in lext if statement\n",
    "    tl.save_pickle(ad_CD1, 'd_dff_c_CD1_p2.pickle', d_dff_c_CD1_p2)\n",
    "    tl.save_pickle(ad_CD1, 'd_dff_s_CD1_p2.pickle', d_dff_s_CD1_p2)\n",
    "    tl.save_pickle(ad_CD1, 'd_dff_s_zsc_CD1_p2.pickle', d_dff_s_zsc_CD1_p2)\n",
    "    \n",
    "    tl.save_pickle(ad_C57, 'd_dff_c_C57_p2.pickle', d_dff_c_C57_p2)\n",
    "    tl.save_pickle(ad_C57, 'd_dff_s_C57_p2.pickle', d_dff_s_C57_p2)\n",
    "    tl.save_pickle(ad_C57, 'd_dff_s_zsc_C57_p2.pickle', d_dff_s_zsc_C57_p2)\n",
    "\n",
    "    tl.save_pickle(ad_Setd1, 'd_dff_c_Setd1_p2.pickle', d_dff_c_Setd1_p2)\n",
    "    tl.save_pickle(ad_Setd1, 'd_dff_s_Setd1_p2.pickle', d_dff_s_Setd1_p2)\n",
    "    tl.save_pickle(ad_Setd1, 'd_dff_s_zsc_Setd1_p2.pickle', d_dff_s_zsc_Setd1_p2)\n",
    "\n",
    "    tl.save_pickle(ad_Df16, 'd_dff_c_Df16_p2.pickle', d_dff_c_Df16_p2)\n",
    "    tl.save_pickle(ad_Df16, 'd_dff_s_Df16_p2.pickle', d_dff_s_Df16_p2)\n",
    "    tl.save_pickle(ad_Df16, 'd_dff_s_zsc_Df16_p2.pickle', d_dff_s_zsc_Df16_p2)\n",
    "    \n",
    "if (1):\n",
    "    d_dff_c_CD1_p2 = tl.load_pickle(ad_CD1, 'd_dff_c_CD1_p2.pickle')\n",
    "    d_dff_s_CD1_p2 = tl.load_pickle(ad_CD1, 'd_dff_s_CD1_p2.pickle')\n",
    "    d_dff_s_zsc_CD1_p2 = tl.load_pickle(ad_CD1, 'd_dff_s_zsc_CD1_p2.pickle')\n",
    "    \n",
    "    d_dff_c_C57_p2 = tl.load_pickle(ad_C57, 'd_dff_c_C57_p2.pickle')\n",
    "    d_dff_s_C57_p2 = tl.load_pickle(ad_C57, 'd_dff_s_C57_p2.pickle')\n",
    "    d_dff_s_zsc_C57_p2 = tl.load_pickle(ad_C57, 'd_dff_s_zsc_C57_p2.pickle')\n",
    "\n",
    "    d_dff_c_Setd1_p2 = tl.load_pickle(ad_Setd1, 'd_dff_c_Setd1_p2.pickle')\n",
    "    d_dff_s_Setd1_p2 = tl.load_pickle(ad_Setd1, 'd_dff_s_Setd1_p2.pickle')\n",
    "    d_dff_s_zsc_Setd1_p2 = tl.load_pickle(ad_Setd1, 'd_dff_s_zsc_Setd1_p2.pickle')\n",
    "\n",
    "    d_dff_c_Df16_p2 = tl.load_pickle(ad_Df16, 'd_dff_c_Df16_p2.pickle')\n",
    "    d_dff_s_Df16_p2 = tl.load_pickle(ad_Df16, 'd_dff_s_Df16_p2.pickle')\n",
    "    d_dff_s_zsc_Df16_p2 = tl.load_pickle(ad_Df16, 'd_dff_s_zsc_Df16_p2.pickle')\n",
    "    \n",
    "if (0):\n",
    "    d_dff_C57_s = tl.load_pickle(ad_C57, 'dict_dff_s.pickle')\n",
    "    d_dff_C57_s_zsc = tl.load_pickle(ad_C57, 'dict_dff_s_zsc.pickle')\n",
    "\n",
    "    d_dff_CD1_s = tl.load_pickle(ad_CD1, 'dict_dff_s.pickle')\n",
    "    d_dff_CD1_s_zsc = tl.load_pickle(ad_CD1, 'dict_dff_s_zsc.pickle')\n",
    "\n",
    "    d_dff_Setd1_s = tl.load_pickle(ad_Setd1, 'dict_dff_s.pickle')\n",
    "    d_dff_Setd1_s_zsc = tl.load_pickle(ad_Setd1, 'dict_dff_s_zsc.pickle')\n",
    "\n",
    "    d_dff_Df16_s = tl.load_pickle(ad_Df16, 'dict_dff_s.pickle')\n",
    "    d_dff_Df16_s_zsc = tl.load_pickle(ad_Df16, 'dict_dff_s_zsc.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### calculate firing fwhm\n",
    "####fwhm_run\n",
    "\n",
    "def run_fwhm_rise_tm(d_s_zsc, d_nsp, d_c, zsc_th=3, perc=0.25):\n",
    "    d_rise_tm = {}\n",
    "    d_rise_tm_pos = {}\n",
    "    d_fwhm = {}\n",
    "    d_fwhm_pos_back = {}\n",
    "    d_fwhm_pos_fwd = {}\n",
    "    d_no_spk_in_fwhm = {}\n",
    "    for k in d_s_zsc.keys():\n",
    "        print(k)\n",
    "        nsp_ids = d_nsp[k]\n",
    "        s_zsc = d_s_zsc[k]\n",
    "        s_zsc = s_zsc[nsp_ids>0,:]\n",
    "        c = d_c[k]\n",
    "        c = c[nsp_ids>0,:]\n",
    "        d_rise_tm[k], d_rise_tm_pos[k] = tl.calc_rise_tm(c, s_zsc, zsc_th=3)\n",
    "        d_fwhm_pos_back[k], d_fwhm_pos_fwd[k], d_fwhm[k], d_no_spk_in_fwhm[k] = tl.calc_fwhm_spikes(c, s_zsc, zsc_th=zsc_th, perc=perc)\n",
    "    return d_rise_tm, d_rise_tm_pos, d_fwhm_pos_back, d_fwhm_pos_fwd, d_fwhm, d_no_spk_in_fwhm\n",
    "    \n",
    "\n",
    "d_rise_tm_C57, d_rise_tm_pos_C57, d_fwhm_pos_back_C57, d_fwhm_pos_fwd_C57, d_fwhm_C57, d_no_spk_in_fwhm_C57 = run_fwhm_rise_tm(\n",
    "    d_dff_s_zsc_C57_p2, d_nspIDs_C57, d_dff_C57, zsc_th=5, perc=0.75)\n",
    "print('c57 done')\n",
    "\n",
    "d_rise_tm_Setd1, d_rise_tm_pos_Setd1, d_fwhm_pos_back_Setd1, d_fwhm_pos_fwd_Setd1, d_fwhm_Setd1, d_no_spk_in_fwhm_Setd1 = run_fwhm_rise_tm(\n",
    "    d_dff_s_zsc_Setd1_p2, d_nspIDs_Setd1, d_dff_Setd1, zsc_th=5, perc=0.75)\n",
    "print('setd1 done')\n",
    "\n",
    "d_rise_tm_CD1, d_rise_tm_pos_CD1, d_fwhm_pos_back_CD1, d_fwhm_pos_fwd_CD1, d_fwhm_CD1, d_no_spk_in_fwhm_CD1 = run_fwhm_rise_tm(\n",
    "    d_dff_s_zsc_CD1_p2, d_nspIDs_CD1, d_dff_CD1, zsc_th=5, perc=0.75)\n",
    "print('CD1 done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "\n",
    "csv_dir = r'Y:\\Data'\n",
    "dd_CD1 = pd.read_csv(os.path.join(csv_dir, r'WT experiments list CD1 v02.csv'))\n",
    "dd_C57 = pd.read_csv(os.path.join(csv_dir, r'WT experiments list C57 v02.csv'))\n",
    "\n",
    "csv_dir1 = r'Y:\\Data'\n",
    "dd_Setd1 = pd.read_csv(os.path.join(csv_dir1, r'SETD1 experiments v02.csv'))\n",
    "\n",
    "\n",
    "csv_dir2 = r'Y:\\Data'\n",
    "dd_Df16 = pd.read_csv(os.path.join(csv_dir2, r'DF16 experiment v01.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Firing rate calculations\n",
    "def calc_frpm(d_s_zsc, d_nsp, zsc_th=5):\n",
    "    d_frpm = {}\n",
    "    for k in d_s_zsc.keys():\n",
    "        s_zsc = np.copy(d_s_zsc[k])\n",
    "        nsp_ids = d_nsp[k]\n",
    "        s_zsc = s_zsc[nsp_ids>0,:]\n",
    "        e = s_zsc >= zsc_th\n",
    "        s_zsc[e] = 1\n",
    "        s_zsc[~e] = 0\n",
    "        s_sum = np.sum(s_zsc, axis=1)\n",
    "        s_sum = s_sum*30*60/s_zsc.shape[1]\n",
    "#         print(np.sum(s_sum)/s_zsc.shape[0])\n",
    "        d_frpm[k] = np.sum(s_sum)/s_zsc.shape[0]\n",
    "    return d_frpm\n",
    "\n",
    "d_frpm_CD1 = calc_frpm(d_dff_s_zsc_CD1_p2, d_nspIDs_CD1)\n",
    "d_frpm_C57 = calc_frpm(d_dff_s_zsc_C57_p2, d_nspIDs_C57)\n",
    "d_frpm_Setd1 = calc_frpm(d_dff_s_zsc_Setd1_p2, d_nspIDs_Setd1)\n",
    "d_frpm_Df16 = calc_frpm(d_dff_s_zsc_Df16_p2, d_nspIDs_Df16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIV:  11\n",
      "DIV:  12\n",
      "DIV:  13\n",
      "DIV:  14\n",
      "DIV:  15\n",
      "DIV:  16\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  28\n",
      "DIV:  29\n",
      "DIV:  25\n",
      "DIV:  16\n",
      "DIV:  17\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  25\n",
      "DIV:  26\n",
      "DIV:  28\n",
      "DIV:  29\n",
      "DIV:  30\n",
      "DIV:  31\n",
      "DIV:  32\n",
      "DIV:  33\n",
      "DIV:  16\n",
      "DIV:  17\n",
      "DIV:  18\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  26\n",
      "DIV:  28\n",
      "DIV:  30\n",
      "DIV:  31\n",
      "DIV:  33\n",
      "DIV:  16\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  32\n",
      "DIV:  35\n"
     ]
    }
   ],
   "source": [
    "d_frpm_in_DIVs_CD1 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_CD1, d_frpm_CD1)\n",
    "d_frpm_in_DIVs_CD1_in_C57 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_CD1_in_C57, d_frpm_C57)\n",
    "d_frpm_in_DIVs_C57 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_C57, d_frpm_C57)\n",
    "d_frpm_in_DIVs_Setd1 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_Setd1, d_frpm_Setd1)\n",
    "d_frpm_in_DIVs_Df16 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_Df16, d_frpm_Df16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIV:  11\n",
      "DIV:  12\n",
      "DIV:  13\n",
      "DIV:  14\n",
      "DIV:  15\n",
      "DIV:  16\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  28\n",
      "DIV:  29\n",
      "DIV:  25\n",
      "DIV:  16\n",
      "DIV:  17\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  25\n",
      "DIV:  26\n",
      "DIV:  28\n",
      "DIV:  29\n",
      "DIV:  30\n",
      "DIV:  31\n",
      "DIV:  32\n",
      "DIV:  33\n",
      "DIV:  16\n",
      "DIV:  17\n",
      "DIV:  18\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  26\n",
      "DIV:  28\n",
      "DIV:  30\n",
      "DIV:  31\n",
      "DIV:  33\n",
      "DIV:  16\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  32\n",
      "DIV:  35\n",
      "DIV:  11\n",
      "DIV:  12\n",
      "DIV:  13\n",
      "DIV:  14\n",
      "DIV:  15\n",
      "DIV:  16\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  28\n",
      "DIV:  29\n",
      "DIV:  25\n",
      "DIV:  16\n",
      "DIV:  17\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  25\n",
      "DIV:  26\n",
      "DIV:  28\n",
      "DIV:  29\n",
      "DIV:  30\n",
      "DIV:  31\n",
      "DIV:  32\n",
      "DIV:  33\n",
      "DIV:  16\n",
      "DIV:  17\n",
      "DIV:  18\n",
      "DIV:  19\n",
      "DIV:  20\n",
      "DIV:  21\n",
      "DIV:  22\n",
      "DIV:  23\n",
      "DIV:  24\n",
      "DIV:  26\n",
      "DIV:  28\n",
      "DIV:  30\n",
      "DIV:  31\n",
      "DIV:  33\n"
     ]
    }
   ],
   "source": [
    "def sort_into_DIVs(d_k_in_DIVs, d_prop, max_tm=4000):\n",
    "    d_prop_in_DIVs = {}\n",
    "    for dk in d_k_in_DIVs.keys():\n",
    "        print('DIV: ', dk)\n",
    "        k_in_dk = d_k_in_DIVs[dk]\n",
    "        list_vals = []\n",
    "        for k in k_in_dk:\n",
    "            d = d_prop[k]\n",
    "            mn_list = []\n",
    "            for k1 in d.keys():\n",
    "#                 print(np.max(d[k1]))\n",
    "                if (np.max(d[k1]) <= max_tm):\n",
    "                    mn_list.append(np.mean(d[k1]))\n",
    "            list_vals.append(np.mean(mn_list))\n",
    "        d_prop_in_DIVs[dk] = list_vals\n",
    "    return d_prop_in_DIVs\n",
    "\n",
    "d_fwhm_in_DIVs_CD1 = sort_into_DIVs(d_k_in_DIVs_CD1, d_fwhm_CD1, max_tm=4000)\n",
    "d_fwhm_in_DIVs_CD1_in_C57 = sort_into_DIVs(d_k_in_DIVs_CD1_in_C57, d_fwhm_C57, max_tm=4000)\n",
    "d_fwhm_in_DIVs_C57 = sort_into_DIVs(d_k_in_DIVs_C57, d_fwhm_C57, max_tm=4000)\n",
    "d_fwhm_in_DIVs_Setd1 = sort_into_DIVs(d_k_in_DIVs_Setd1, d_fwhm_Setd1, max_tm=4000)\n",
    "d_fwhm_in_DIVs_Df16 = sort_into_DIVs(d_k_in_DIVs_Df16, d_fwhm_Df16, max_tm=4000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Correlations calculations for All\n",
    "\n",
    "if(1):\n",
    "    print('#### CD1')\n",
    "    d_mn_pwc_CD1, d_mn_pwc_CD1_inter, d_mn_pwc_CD1_intra = tl.calc_pwc_mn(d_k_in_DIVs_CD1, d_dff_CD1,\n",
    "                                                                          d_nspIDs_CD1, dff_cut=0.0, norm_corr = False)\n",
    "\n",
    "    print('#### C57')\n",
    "    d_mn_pwc_C57, d_mn_pwc_C57_inter, d_mn_pwc_C57_intra = tl.calc_pwc_mn(d_k_in_DIVs_C57, d_dff_C57, \n",
    "                                                                       d_nspIDs_C57, dff_cut=0.0, norm_corr = False)\n",
    "    print('#### Setd1')\n",
    "    d_mn_pwc_Setd1, d_mn_pwc_Setd1_inter, d_mn_pwc_Setd1_intra = tl.calc_pwc_mn(d_k_in_DIVs_Setd1, d_dff_Setd1, \n",
    "                                                                       d_nspIDs_Setd1, dff_cut=0.0, norm_corr = False)\n",
    "    print('#### Df16')\n",
    "    d_mn_pwc_Df16, d_mn_pwc_Df16_inter, d_mn_pwc_Df16_intra = tl.calc_pwc_mn(d_k_in_DIVs_Df16, d_dff_Df16, \n",
    "                                                                       d_nspIDs_Df16, dff_cut=0.0, norm_corr = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_star(p):\n",
    "    if(p<0.0001):\n",
    "        return '****'\n",
    "    elif(p<0.001):\n",
    "        return '***'\n",
    "    elif(p<0.01):\n",
    "        return '**'\n",
    "    elif(p<0.05):\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'n.s.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###Phase\n",
    "### divide DIVs into 3 phases (pools) \n",
    "to_pool1 = list(np.arange(0,18))\n",
    "to_pool2 = list(np.arange(18,27))\n",
    "to_pool3 = list(np.arange(27,30))\n",
    "\n",
    "d_1 = d_mn_pwc_CD1_inter.copy()\n",
    "A = set(d_1.keys())\n",
    "\n",
    "dat1 = []\n",
    "dat2 = []\n",
    "dat3 = []\n",
    "\n",
    "for i in A:\n",
    "    if (i in to_pool1):\n",
    "        dat1.extend(d_1[i])\n",
    "    if (i in to_pool2):\n",
    "        dat2.extend(d_1[i])\n",
    "    if (i in to_pool3):\n",
    "        dat3.extend(d_1[i])\n",
    "        \n",
    "df_dat = []\n",
    "for ii in range(len(dat1)):\n",
    "    df_dat.append({'c':dat1[ii], 'state': 'p1'})\n",
    "for ii in range(len(dat2)):\n",
    "    df_dat.append({'c':dat2[ii], 'state': 'p2'})\n",
    "for ii in range(len(dat3)):\n",
    "    df_dat.append({'c':dat3[ii], 'state': 'p3'})\n",
    "\n",
    "tdf = pd.DataFrame(df_dat)\n",
    "tdf.to_pickle(r'E:\\Raju\\MoNNet\\Global_pwc_tdf_01.pkl')\n",
    "\n",
    "print('Global PWC pvals')\n",
    "an = f_oneway(dat1, dat2, dat3)\n",
    "print(an)\n",
    "tukey = pairwise_tukeyhsd(endog=tdf['c'],\n",
    "                          groups=tdf['state'],\n",
    "                          alpha=0.05)\n",
    "print('Tukey')\n",
    "print(tukey)\n",
    "print(print_star(tukey.pvalues[0]))\n",
    "print(print_star(tukey.pvalues[1]))\n",
    "print(print_star(tukey.pvalues[2]))\n",
    "\n",
    "print('p1 mean:', np.mean(dat1), ' , p1 std:', np.std(dat1))\n",
    "print('p2 mean:', np.mean(dat2), ' , p2 std:', np.std(dat2))\n",
    "print('p3 mean:', np.mean(dat3), ' , p3 std:', np.std(dat3))\n",
    "print('\\n')\n",
    "print('pairwise ttest')\n",
    "t=stats.ttest_ind(dat1, dat2)\n",
    "print('p1 and p2 pval: ', t.pvalue, print_star(t.pvalue))\n",
    "\n",
    "t=stats.ttest_ind(dat2, dat3)\n",
    "print('p2 and p3 pval: ', t.pvalue, print_star(t.pvalue))\n",
    "\n",
    "t=stats.ttest_ind(dat1, dat3)\n",
    "print('p1 and p3 pval: ', t.pvalue, print_star(t.pvalue))\n",
    "\n",
    "fig = plt.figure(figsize=(2,3))\n",
    "ax = fig.add_axes([0., 0., 1., 1.])\n",
    "ax.margins(0.008)\n",
    "\n",
    "ax = sns.barplot(x=\"state\", y=\"c\", data=tdf, ax=ax)\n",
    "plt.ylim(0,1.05)\n",
    "# plt.xticks(np.arange(10, 31, step=5))\n",
    "plt.yticks(np.arange(0, 1.1, step=.5))\n",
    "\n",
    "fig_dir = r'E:\\Raju\\MoNNet\\F1Rel\\phases_final'\n",
    "save_fig = True\n",
    "fig_fn = os.path.join(fig_dir,'CD1_PWC_Inter_phases')\n",
    "\n",
    "if (save_fig):\n",
    "    print(fig_fn + '.png')\n",
    "    fig.savefig(fig_fn + '.png', dpi=200, bbox_inches=\"tight\")            \n",
    "    print(fig_fn + '.eps')\n",
    "    fig.savefig(fig_fn + '.eps', dpi=200, bbox_inches=\"tight\")            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Pairwise Correlations plots ############\n",
    "#### Inter ###\n",
    "\n",
    "pdeg = 4\n",
    "lw = 1\n",
    "lwp = .5\n",
    "psz=2\n",
    "fig = plt.figure(figsize=(2,2))\n",
    "ax = fig.add_axes([0., 0., 1., 1.])\n",
    "ax.margins(0.008)\n",
    "fig_dir = r'E:\\Raju\\MoNNet\\Setd1FigRel'\n",
    "save_fig = False\n",
    "fig_fn = os.path.join(fig_dir,'CD1_C57_Setd1_Df16_PWC_Inter_pdeg'+ str(pdeg))\n",
    "\n",
    "                      \n",
    "                      \n",
    "print(d_mn_pwc_CD1_inter.keys())\n",
    "d, d_std = tl.gen_mn_of_mn(d_mn_pwc_CD1_inter)\n",
    "d = tl.sort_dict(d)\n",
    "d_std = tl.sort_dict(d_std)\n",
    "ax.errorbar(d.keys(), d.values(), yerr=d_std.values(), fmt='o', \n",
    "            color='gray', label='CD1 WT', linewidth=lwp, markersize=psz)\n",
    "x,y = tl.gen_poly_fit(d,deg=pdeg)\n",
    "ax.plot(x, y, color='gray', linewidth=lw)\n",
    "\n",
    "d, d_std = tl.gen_mn_of_mn(d_mn_pwc_C57_inter)\n",
    "d = tl.sort_dict(d)\n",
    "d_std = tl.sort_dict(d_std)\n",
    "ax.errorbar(d.keys(), d.values(), yerr=d_std.values(), fmt='o', \n",
    "            label='C57 WT', color='C0', linewidth=lwp, markersize=psz)\n",
    "\n",
    "x,y = tl.gen_poly_fit(d,deg=pdeg)\n",
    "ax.plot(x, y, color='C0', linewidth=lw)\n",
    "\n",
    "\n",
    "\n",
    "d, d_std = tl.gen_mn_of_mn(d_mn_pwc_Setd1_inter)\n",
    "d = tl.sort_dict(d)\n",
    "d_std = tl.sort_dict(d_std)\n",
    "ax.errorbar(d.keys(), d.values(), yerr=d_std.values(), fmt='o', \n",
    "            label='Setd1', color='C1', linewidth=lwp, markersize=psz)\n",
    "x,y = tl.gen_poly_fit(d,deg=pdeg)\n",
    "ax.plot(x, y, color='C1', linewidth=lw)\n",
    "\n",
    "\n",
    "\n",
    "d, d_std = tl.gen_mn_of_mn(d_mn_pwc_Df16_inter)\n",
    "d = tl.sort_dict(d)\n",
    "d_std = tl.sort_dict(d_std)\n",
    "ax.errorbar(d.keys(), d.values(), yerr=d_std.values(), fmt='o', \n",
    "            label='Df16', color='green', linewidth=lwp, markersize=psz)\n",
    "x,y = tl.gen_poly_fit(d,deg=pdeg)\n",
    "ax.plot(x, y, color='green', linewidth=lw)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.legend(loc=0)\n",
    "plt.xlim(10,36)\n",
    "plt.ylim(0,1.05)\n",
    "plt.xticks(np.arange(10, 36, step=5))\n",
    "# plt.grid()\n",
    "# ax.set_xlabel('DIVs', fontsize=18)\n",
    "# ax.set_ylabel('Avg Pairwise Correlation', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "if (save_fig):\n",
    "    print(fig_fn + '.png')\n",
    "    fig.savefig(fig_fn + '.png', dpi=200, bbox_inches=\"tight\")            \n",
    "    print(fig_fn + '.eps')\n",
    "    fig.savefig(fig_fn + '.eps', dpi=200, bbox_inches=\"tight\")            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ Modularity analysis\n",
    "if(1):\n",
    "    ccut = 0.5\n",
    "#     pool = mp.Pool(processes=5)\n",
    "    print('#### CD1')\n",
    "    d_no_comm_CD1, d_mod_CD1 = tl.calc_mod_comm(d_k_in_DIVs_CD1, d_dff_CD1,\n",
    "                                             d_nspIDs_CD1, d_com_CD1, dff_cut=0.1, norm_corr = False, ccut=ccut, binary=False)\n",
    "\n",
    "    print('#### C57')\n",
    "    d_no_comm_C57, d_mod_C57 = tl.calc_mod_comm(d_k_in_DIVs_C57, d_dff_C57,\n",
    "                                             d_nspIDs_C57, d_com_C57, dff_cut=0.1, norm_corr = False, ccut=ccut, binary=False)\n",
    "\n",
    "    print('#### Setd1')\n",
    "    d_no_comm_Setd1, d_mod_Setd1 = tl.calc_mod_comm(d_k_in_DIVs_Setd1, d_dff_Setd1,\n",
    "                                             d_nspIDs_Setd1, d_com_Setd1, dff_cut=0.1, norm_corr = False, ccut=ccut, binary=False)\n",
    "\n",
    "    print('#### Df16')\n",
    "    d_no_comm_Df16, d_mod_Df16 = tl.calc_mod_comm(d_k_in_DIVs_Df16, d_dff_Df16,\n",
    "                                             d_nspIDs_Df16, d_com_Df16, dff_cut=0.1, norm_corr = False, ccut=ccut, binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save eps for short list\n",
    "### for Setd1\n",
    "##### Plot graphs with labelled communities and save them, limited number to 9 distinct colors/clusters\n",
    "if(1): \n",
    "    fig_dir = r'X:\\People\\Raju\\F1Rel\\selected_graphs'\n",
    "    save_fig = True\n",
    "\n",
    "    louvian = True\n",
    "    node_size = 12\n",
    "    d_nspIDs = d_nspIDs_CD1\n",
    "    d_dff = d_dff_CD1\n",
    "    d_labIm = d_labIm_CD1\n",
    "    d_com = d_com_CD1\n",
    "    d_k_in_DIVs = d_k_in_DIVs_CD1\n",
    "    cmap_ = plt.cm.Blues\n",
    "    dff_cut = 0.1\n",
    "    edge_wt = 1\n",
    "    alpha=0.75\n",
    "    vmin = 0.3\n",
    "    vmax = 1.0\n",
    "    for dk in d_k_in_DIVs.keys():\n",
    "        print('DIV: ', dk)\n",
    "        k_in_dk = d_k_in_DIVs[dk]\n",
    "        list_mod = []\n",
    "        list_no_comm = []\n",
    "        for k in k_in_dk:\n",
    "#             if (k in [30, 70,170, 268]):\n",
    "            if (k in [70, 170, 276]):\n",
    "                print('DIV: ', dk, ', key: ', k)\n",
    "                tmp_dat = np.copy(d_dff[k])\n",
    "                nsp_ids = d_nspIDs[k]\n",
    "                nsp_ids_uniq = np.unique(nsp_ids)\n",
    "                if (0 in nsp_ids_uniq):\n",
    "                    tmp_dat_filt = tmp_dat[nsp_ids>0,:]\n",
    "                    nsp_ids_filt = nsp_ids[nsp_ids>0]\n",
    "                else:\n",
    "                    tmp_dat_filt = np.copy(tmp_dat)\n",
    "                    nsp_ids_filt = np.copy(nsp_ids)\n",
    "                tmp_dat_filt[tmp_dat_filt<dff_cut] = 0\n",
    "                r = np.corrcoef(tmp_dat_filt)\n",
    "                r = np.nan_to_num(r)\n",
    "                np.fill_diagonal(r,0)\n",
    "\n",
    "                im_lab = d_labIm[k]\n",
    "                coms = d_com[k]\n",
    "                coms = coms[nsp_ids>0,:]\n",
    "                index = list(range(1,coms.shape[0]+1))\n",
    "                cord = {}\n",
    "                for i in range(1,coms.shape[0]+1):\n",
    "                    cord[i] = [coms[i-1,1],coms[i-1,0]]\n",
    "\n",
    "                g=nx.Graph()\n",
    "                for i in range(len(index)):\n",
    "                    g.add_node(index[i])\n",
    "                for i in range(len(index)):\n",
    "                    for j in range(i+1, len(index)):\n",
    "                        if (r[i,j] > 0):\n",
    "                            g.add_edge(index[i],index[j],weight=r[i,j])\n",
    "\n",
    "                if (louvian):\n",
    "                    clusters = community.best_partition(g)\n",
    "                    size = float(len(set(clusters.values())))\n",
    "                    mod = community.modularity(clusters, g, weight='weight')\n",
    "                else:\n",
    "                    gi = pyintergraph.nx2igraph(g)\n",
    "                    clusters = community.best_partition(gi)\n",
    "                    a = clusters.communities\n",
    "                    size = len(a)\n",
    "                    mod = 0\n",
    "                print('No of clusters: ', len(set(clusters.values())))\n",
    "                colors = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
    "                fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 9), sharex=True, sharey=True)\n",
    "                im_lab_ = np.copy(im_lab)\n",
    "                im_lab_[im_lab_>0] = 10\n",
    "                ax[0].imshow(im_lab_, cmap='Greys', vmin=0, vmax=40)\n",
    "                ax[1].imshow(im_lab_, cmap='Greys', vmin=0, vmax=40)\n",
    "\n",
    "                c_id = 0\n",
    "                for comm in set(clusters.values()):\n",
    "                    list_nodes = [nodes for nodes in clusters.keys()\n",
    "                                                if clusters[nodes] == comm]\n",
    "                    if (len(list_nodes)==1):\n",
    "                        nx.draw_networkx_nodes(g, cord, list_nodes, node_size = node_size,\n",
    "                                                    node_color = 'black', ax=ax[0])\n",
    "                    if (len(list_nodes)>1):\n",
    "                    #     print(list_nodes)\n",
    "                        nx.draw_networkx_nodes(g, cord, list_nodes, node_size = node_size,\n",
    "                                                    node_color = colors[c_id], ax=ax[0])\n",
    "                        c_id = c_id + 1\n",
    "                        if(c_id>8):\n",
    "                            c_id=0\n",
    "                            print('More than 9 clusters!')\n",
    "                # plot edges graph\n",
    "                g_edges = sorted(g.edges(data=True), key=lambda t: t[2].get('weight', 1))\n",
    "                g_colors = []\n",
    "                for e in g_edges:\n",
    "                    c = cmap_((e[2]['weight'] - vmin)/(vmax - vmin))\n",
    "                    g_colors.append(c)\n",
    "                    nx.draw_networkx_edges(g,pos=cord, edgelist=[e], with_labels=False, \n",
    "                                           width=edge_wt, edge_color=c, alpha=alpha, ax=ax[1], node_size=node_size)\n",
    "    #             nx.draw_networkx_nodes(g,pos=cord,node_size=node_size, ax=ax[1])\n",
    "                c_id=0\n",
    "                for comm in set(clusters.values()):\n",
    "                    list_nodes = [nodes for nodes in clusters.keys()\n",
    "                                                if clusters[nodes] == comm]\n",
    "                    if (len(list_nodes)==1):\n",
    "                        nx.draw_networkx_nodes(g, cord, list_nodes, node_size = node_size,\n",
    "                                                    node_color = 'black', ax=ax[1])\n",
    "                    if (len(list_nodes)>1):\n",
    "                    #     print(list_nodes)\n",
    "                        nx.draw_networkx_nodes(g, cord, list_nodes, node_size = node_size,\n",
    "                                                    node_color = colors[c_id], ax=ax[1])\n",
    "                        c_id = c_id + 1\n",
    "                        if(c_id>8):\n",
    "                            c_id=0\n",
    "                            print('More than 9 clusters!')\n",
    "\n",
    "                colors_unscaled=[tuple(map(lambda x: (vmax - vmin)*x + vmin, y)) for y in g_colors]\n",
    "                plt.ioff()\n",
    "                fig_temp,ax_temp = plt.subplots()\n",
    "                heatmap = plt.pcolor(colors_unscaled,cmap=cmap_, vmin=vmin, vmax=vmax)\n",
    "                plt.close(fig_temp)\n",
    "                plt.ion()\n",
    "                cbar = plt.colorbar(heatmap, ax=ax, shrink=0.5, use_gridspec=True)\n",
    "                cbar.ax.set_ylabel('Corr',labelpad=15,rotation=270)\n",
    "                cbar.set_ticks(np.arange(vmin, vmax+1, 0.1))\n",
    "\n",
    "                plt.show()\n",
    "                if (save_fig):\n",
    "                    nc = len(set(clusters.values()))\n",
    "                    fig_fn = os.path.join(fig_dir,'DIV' + str(dk) + '_Louv_k' + str(k) + '_nc' \n",
    "                                          + str(nc) + '.png')\n",
    "                    print(fig_fn)\n",
    "                    fig.savefig(fig_fn, dpi=150, bbox_inches=\"tight\")\n",
    "                    fig_fn = os.path.join(fig_dir,'DIV' + str(dk) + '_Louv_k' + str(k) + '_nc' \n",
    "                                          + str(nc) + '.eps')\n",
    "                    fig.savefig(fig_fn, dpi=150, bbox_inches=\"tight\", format='eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### generate graphs (for network efficiency calculations) for all and store as dict files\n",
    "if(1):\n",
    "    d_g_CD1 = tl.gen_graph_collection(d_k_in_DIVs_CD1, d_dff_CD1, \n",
    "                                   d_nspIDs_CD1, d_com_CD1, dff_cut=0.05, norm_corr = False, ccut=0.5, binary=False)\n",
    "\n",
    "    d_g_C57 = tl.gen_graph_collection(d_k_in_DIVs_C57, d_dff_C57, \n",
    "                                   d_nspIDs_C57, d_com_C57, dff_cut=0.05, norm_corr = False, ccut=0.5, binary=False)\n",
    "\n",
    "    d_g_Setd1 = tl.gen_graph_collection(d_k_in_DIVs_Setd1, d_dff_Setd1, \n",
    "                                   d_nspIDs_Setd1, d_com_Setd1, dff_cut=0.05, norm_corr = False, ccut=0.5, binary=False)\n",
    "\n",
    "    d_g_Df16 = tl.gen_graph_collection(d_k_in_DIVs_Df16, d_dff_Df16, \n",
    "                                   d_nspIDs_Df16, d_com_Df16, dff_cut=0.05, norm_corr = False, ccut=0.5, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Global Network Efficiency\n",
    "pool = mp.Pool(processes=96)\n",
    "pres_geff_CD1 = [pool.apply(tl.calc_global_eff_worker, args=(k,d_g_CD1[k])) for k in d_g_CD1.keys()]\n",
    "print(pres_geff_CD1)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "pool = mp.Pool(processes=96)\n",
    "pres_geff_CD1_in_C57 = [pool.apply(tl.calc_global_eff_worker, args=(k,d_g_CD1_in_C57[k])) for k in d_g_CD1_in_C57.keys()]\n",
    "print(pres_geff_CD1_in_C57)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "pool = mp.Pool(processes=96)\n",
    "pres_geff_C57 = [pool.apply(tl.calc_global_eff_worker, args=(k,d_g_C57[k])) for k in d_g_C57.keys()]\n",
    "print(pres_geff_C57)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "pool = mp.Pool(processes=96)\n",
    "pres_geff_Setd1 = [pool.apply(tl.calc_global_eff_worker, args=(k,d_g_Setd1[k])) for k in d_g_Setd1.keys()]\n",
    "print(pres_geff_Setd1)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "pool = mp.Pool(processes=96)\n",
    "pres_geff_Df16 = [pool.apply(tl.calc_global_eff_worker, args=(k,d_g_Df16[k])) for k in d_g_Df16.keys()]\n",
    "print(pres_geff_Df16)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "### Local Network efficiency\n",
    "if (pool):\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "pool = mp.Pool(processes=80)\n",
    "pres_leff_CD1 = [pool.apply(tl.calc_local_eff_worker, args=(k,d_g_CD1[k])) for k in d_g_CD1.keys()]\n",
    "print(pres_leff_CD1)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "if (pool):\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "pool = mp.Pool(processes=80)\n",
    "pres_leff_C57 = [pool.apply(tl.calc_local_eff_worker, args=(k,d_g_C57[k])) for k in d_g_C57.keys()]\n",
    "print(pres_leff_C57)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "if (pool):\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "pool = mp.Pool(processes=80)\n",
    "pres_leff_Setd1 = [pool.apply(tl.calc_local_eff_worker, args=(k,d_g_Setd1[k])) for k in d_g_Setd1.keys()]\n",
    "print(pres_leff_Setd1)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "if (pool):\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "pool = mp.Pool(processes=80)\n",
    "pres_leff_Df16 = [pool.apply(tl.calc_local_eff_worker, args=(k,d_g_Df16[k])) for k in d_g_Df16.keys()]\n",
    "print(pres_leff_Df16)\n",
    "print('Done')\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### save the calculated values\n",
    "### this needs fixing\n",
    "\n",
    "if (0):\n",
    "    tl.save_pickle(ad_CD1, 'pres_leff_CD1_ccut05_weighted.pickle', pres_leff_CD1)\n",
    "    tl.save_pickle(ad_C57, 'pres_leff_C57_ccut05_weighted.pickle', pres_leff_C57)\n",
    "    tl.save_pickle(ad_Setd1, 'pres_leff_Setd1_ccut05_weighted.pickle', pres_leff_Setd1)\n",
    "    tl.save_pickle(ad_Df16, 'pres_leff_Df16_ccut05_weighted.pickle', pres_leff_Df16)\n",
    "\n",
    "if (0):\n",
    "    tl.save_pickle(ad_CD1, 'pres_geff_CD1_ccut05_weighted.pickle', pres_geff_CD1)\n",
    "    tl.save_pickle(ad_C57, 'pres_geff_C57_ccut05_weighted.pickle', pres_geff_C57)\n",
    "    tl.save_pickle(ad_Setd1, 'pres_geff_Setd1_ccut05_weighted.pickle', pres_geff_Setd1)\n",
    "    tl.save_pickle(ad_Df16, 'pres_geff_Df16_ccut05_weighted.pickle', pres_geff_Df16)\n",
    "\n",
    "if(1):\n",
    "    pres_leff_CD1 = tl.load_pickle(ad_CD1, 'pres_leff_CD1_ccut08.pickle')\n",
    "    pres_leff_C57 = tl.load_pickle(ad_C57, 'pres_leff_C57_ccut08.pickle')\n",
    "    pres_leff_Setd1 = tl.load_pickle(ad_Setd1, 'pres_leff_Setd1_ccut08.pickle')\n",
    "    pres_leff_Df16 = tl.load_pickle(ad_Df16, 'pres_leff_Df16_ccut08.pickle')\n",
    "    print('Done')\n",
    "    \n",
    "if(1):\n",
    "    pres_geff_CD1 = tl.load_pickle(ad_CD1, 'pres_geff_CD1_ccut08.pickle')\n",
    "    pres_geff_C57 = tl.load_pickle(ad_C57, 'pres_geff_C57_ccut08.pickle')\n",
    "    pres_geff_Setd1 = tl.load_pickle(ad_Setd1, 'pres_geff_Setd1_ccut08.pickle')\n",
    "    pres_geff_Df16 = tl.load_pickle(ad_Df16, 'pres_geff_Df16_ccut08.pickle')\n",
    "    print('Done')\n",
    "\n",
    "if(0):\n",
    "    pres_leff_CD1 = tl.load_pickle(ad_CD1, 'pres_leff_CD1_ccut05_weighted.pickle')\n",
    "    pres_leff_C57 = tl.load_pickle(ad_C57, 'pres_leff_C57_ccut05_weighted.pickle')\n",
    "    pres_leff_Setd1 = tl.load_pickle(ad_Setd1, 'pres_leff_Setd1_ccut05_weighted.pickle')\n",
    "    pres_leff_Df16 = tl.load_pickle(ad_Df16, 'pres_leff_Df16_ccut05_weighted.pickle')\n",
    "    \n",
    "if(0):\n",
    "    pres_geff_CD1 = tl.load_pickle(ad_CD1, 'pres_geff_CD1_ccut05_weighted.pickle')\n",
    "    pres_geff_C57 = tl.load_pickle(ad_C57, 'pres_geff_C57_ccut05_weighted.pickle')\n",
    "    pres_geff_Setd1 = tl.load_pickle(ad_Setd1, 'pres_geff_Setd1_ccut05_weighted.pickle')\n",
    "    pres_geff_Df16 = tl.load_pickle(ad_Df16, 'pres_geff_Df16_ccut05_weighted.pickle')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pool values back from pool\n",
    "\n",
    "d_geff_CD1 = tl.convert_pool_dict(pres_geff_CD1)    \n",
    "d_geff_in_DIVs_CD1 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_CD1, d_geff_CD1)\n",
    "\n",
    "d_geff_C57 = tl.convert_pool_dict(pres_geff_C57)    \n",
    "d_geff_in_DIVs_C57 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_C57, d_geff_C57)\n",
    "\n",
    "d_geff_Setd1 = tl.convert_pool_dict(pres_geff_Setd1)    \n",
    "d_geff_in_DIVs_Setd1 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_Setd1, d_geff_Setd1)\n",
    "\n",
    "d_geff_Df16 = tl.convert_pool_dict(pres_geff_Df16)    \n",
    "d_geff_in_DIVs_Df16 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_Df16, d_geff_Df16)\n",
    "\n",
    "\n",
    "### Plot global efficiency, pool values back from pool\n",
    "\n",
    "d_leff_CD1 = tl.convert_pool_dict(pres_leff_CD1)    \n",
    "d_leff_in_DIVs_CD1 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_CD1, d_leff_CD1)\n",
    "\n",
    "d_leff_C57 = tl.convert_pool_dict(pres_leff_C57)    \n",
    "d_leff_in_DIVs_C57 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_C57, d_leff_C57)\n",
    "\n",
    "d_leff_Setd1 = tl.convert_pool_dict(pres_leff_Setd1)    \n",
    "d_leff_in_DIVs_Setd1 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_Setd1, d_leff_Setd1)\n",
    "\n",
    "d_leff_Df16 = tl.convert_pool_dict(pres_leff_Df16)    \n",
    "d_leff_in_DIVs_Df16 = tl.sort_graph_props_in_DIVs(d_k_in_DIVs_Df16, d_leff_Df16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality\n",
    "# modularity\n",
    "# firing rates\n",
    "def calc_pca(dict_dat, dict_nsp_ids, dff_cut=0.05, min_rows=3):\n",
    "    d_pca_tm = {}\n",
    "#     d_pca_sp = {}\n",
    "    for k in sorted(dict_nsp_ids.keys()):\n",
    "        tmp_dat = np.copy(dict_dat[k])\n",
    "        tmp_dat = np.nan_to_num(tmp_dat)\n",
    "        if (tmp_dat.shape[0] > min_rows):\n",
    "            if(k%50==0):\n",
    "                print(k)\n",
    "            nsp_ids = dict_nsp_ids[k]\n",
    "            nsp_ids_uniq = np.unique(nsp_ids)\n",
    "            if (0 in nsp_ids_uniq):\n",
    "                tmp_dat = tmp_dat[nsp_ids>0,:]\n",
    "                nsp_ids = nsp_ids[nsp_ids>0]\n",
    "            tmp_dat[tmp_dat<dff_cut] = 0\n",
    "#             pca = PCA()\n",
    "#             pca.fit(np.copy(tmp_dat))\n",
    "#             d_pca_sp[k] = pca\n",
    "            \n",
    "            tmp_dat = tmp_dat.T\n",
    "            pca = PCA()\n",
    "            pca.fit(np.copy(tmp_dat))\n",
    "            d_pca_tm[k] = pca\n",
    "        else:\n",
    "            print('Error in ', k, ' No of rows smaller than ', min_rows)\n",
    "    return d_pca_tm\n",
    "\n",
    "def get_pca_dims_var(d_k_in_DIVs, d_pca, cut=0.95, dim=0):\n",
    "    \"\"\"\n",
    "    cut and dim are independet. cut is for dims, dim is for var\n",
    "    \"\"\"\n",
    "    d_pca_dims = {}\n",
    "    d_pca_var = {}\n",
    "    for dk in d_k_in_DIVs.keys():\n",
    "#         print('DIV: ', dk)\n",
    "        k_in_dk = d_k_in_DIVs[dk]\n",
    "        list_vals_dims = []\n",
    "        list_vals_var = []\n",
    "        for k in k_in_dk:\n",
    "            var_r = d_pca[k].explained_variance_ratio_\n",
    "            cumsum_var_r = np.cumsum(var_r)\n",
    "            list_vals_dims.append(sum(cumsum_var_r <= cut))\n",
    "            list_vals_var.append(var_r[dim])\n",
    "#         print(list_vals_dims)\n",
    "#         print(list_vals_var)\n",
    "        d_pca_dims[dk] = np.array(list_vals_dims)\n",
    "        d_pca_var[dk] = np.array(list_vals_var)\n",
    "    return d_pca_dims, d_pca_var\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "if(1):\n",
    "    d_pca_CD1 = calc_pca(d_dff_CD1, d_nspIDs_CD1, dff_cut=0.0, min_rows=3)\n",
    "    d_pca_C57 = calc_pca(d_dff_C57, d_nspIDs_C57, dff_cut=0.0, min_rows=3)\n",
    "    d_pca_Setd1 = calc_pca(d_dff_Setd1, d_nspIDs_Setd1, dff_cut=0.0, min_rows=3)\n",
    "    d_pca_Df16 = calc_pca(d_dff_Df16, d_nspIDs_Df16, dff_cut=0.0, min_rows=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cut = 0.95\n",
    "dim = 0\n",
    "d_pca_dims_CD1, d_pca_var_CD1 = get_pca_dims_var(d_k_in_DIVs_CD1, d_pca_CD1, cut=cut, dim=dim)\n",
    "d_pca_dims_C57, d_pca_var_C57 = get_pca_dims_var(d_k_in_DIVs_C57, d_pca_C57, cut=cut, dim=dim)\n",
    "d_pca_dims_Setd1, d_pca_var_Setd1 = get_pca_dims_var(d_k_in_DIVs_Setd1, d_pca_Setd1, cut=cut, dim=dim)\n",
    "d_pca_dims_Df16, d_pca_var_Df16 = get_pca_dims_var(d_k_in_DIVs_Df16, d_pca_Df16, cut=cut, dim=dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
